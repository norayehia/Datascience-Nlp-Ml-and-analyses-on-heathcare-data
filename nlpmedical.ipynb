{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpmedical.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GrJx-VUdbBoB_wdh0oB2cxdQKnp9JJnB",
      "authorship_tag": "ABX9TyM0w61sojdlc5EzAMHF+iIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norayehia/healthcare-precision-medicine-datascience/blob/main/nlpmedical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkmUIwq5kddT",
        "outputId": "fefb092a-35df-4e5d-b7f6-36642775b776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "import docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xONLO7B8lAeH",
        "outputId": "f92f8d52-1d6c-4147-95c4-1d59dbf4bff0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=f8526b754de99a79c6cea3b327d28918efb3cf246e674607899de8d276a9e7a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Biopsy Report is a Microsoft Word document (in a docx file format). In order for it to be processed by the NLTK package, the document requires to be converted into plain text. In this case, we are using the process method from the docx2txt Python package to do this. We are calling the resulting plain text \"text\""
      ],
      "metadata": {
        "id": "8wh1ebWYlamh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = docx2txt.process('/content/drive/MyDrive/precisiommedicine/Biopsy_Report.docx')"
      ],
      "metadata": {
        "id": "uCbLqv8Bkyvt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "low level"
      ],
      "metadata": {
        "id": "XuTgiAyfm1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1tokknization convert to words Tokenisation breaks large strings into smaller chunks. Let's use NLTK's word_tokenize method to split the text string into words and punctuation. We'll call the resulting list of words and punctuation tokens.\n",
        "tokens = nltk.word_tokenize(text)\n",
        "#to see first 10 words\n",
        "\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAQogKJimqoz",
        "outputId": "ee3531c0-eb7f-4ae5-d97b-60ec108f2e24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'is',\n",
              " 'a',\n",
              " 'biopsy',\n",
              " 'report',\n",
              " '?',\n",
              " 'A',\n",
              " 'biopsy',\n",
              " 'report',\n",
              " 'describes']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2Removing stop words\n",
        "\"\"\"\n",
        "Now we want to remove any stop words (i.e. any commonly used words, such as \"and\") from the list of tokens. We'll call the resulting list of tokens clean_tokens.\n",
        "\n",
        "Note that you don't need to understand the code in the following cell. You can simply reuse it in the future as a template, where you replace tokens with the name of your list of tokens. (But if you're really curious, this is what the following code does: It first creates a copy of tokens called clean_tokens. And then for each token in the list of tokens, if it's in the list of stop words in English, then it is removed.)\n",
        "\"\"\"\n",
        "\n",
        "clean_tokens = tokens[:]\n",
        "for token in tokens:\n",
        "    if token in stopwords.words('english'):\n",
        "        clean_tokens.remove(token)"
      ],
      "metadata": {
        "id": "dGqb2UConEo0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's print out the number of tokens (with and without stop words).\n",
        "print(\"Number of tokens including stop words:  \",len(tokens))\n",
        "print(\"Number of tokens excluding stop words:  \",len(clean_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeTr45kFnSpx",
        "outputId": "1dee19ce-9bdb-4c49-db23-8acf590c8491"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens including stop words:   188\n",
            "Number of tokens excluding stop words:   125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3Frequency distributions\n",
        "\"\"\"Next, we want to investigate the frequency of certain words in the Biopsy Report. We start by generating the frequency distribution of all tokens (including stop words) with the use of NLTK's FreqDist. Let's call this freq.\n",
        "\n",
        "\"\"\"\n",
        "freq = nltk.FreqDist(tokens)\n",
        "\n",
        "#اكتر عشره بيتكرروا\n",
        "freq.most_common(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_q9XLUunS1v",
        "outputId": "2691bfae-305d-409d-d762-9c63cb19951a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 18),\n",
              " ('.', 12),\n",
              " (',', 11),\n",
              " ('of', 9),\n",
              " ('a', 7),\n",
              " ('biopsy', 7),\n",
              " ('and', 5),\n",
              " ('is', 4),\n",
              " ('description', 4),\n",
              " ('A', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#لوعايز اعرف تكرار كلمه نعينه\n",
        "print(\"Frequency of description:  \", freq[\"description\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ehE7Y_PoiHa",
        "outputId": "b9f1c4d4-26de-4ff9-8455-11158c865306"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of description:   4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It can be useful to disregard upper case text and lower case text. Therefore, we can convert upper case text to lower case text. We can do this with the use of Python's lower() method.\n",
        "\n",
        "Let's apply the lower() method to each element in tokens, so that all text is counted as lower case text. We'll call this new list of lower case tokens lowercase_tokens (but you could call it anything you want).\n",
        "\"\"\"\n",
        "#5 convert to lower\n",
        "lowercase_tokens = [t.lower() for t in tokens]\n",
        "lowercase_tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIoAbgIlo9k6",
        "outputId": "228db0c7-3991-496c-af6b-58c48487ab9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what',\n",
              " 'is',\n",
              " 'a',\n",
              " 'biopsy',\n",
              " 'report',\n",
              " '?',\n",
              " 'a',\n",
              " 'biopsy',\n",
              " 'report',\n",
              " 'describes']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get freq of all lower then to know the fre of lower biopsy it repeated 7\n",
        "lowercase_freq = nltk.FreqDist(lowercase_tokens)\n",
        "print(\"Frequency of biopsy:  \", lowercase_freq[\"biopsy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbpUfwZqpxmz",
        "outputId": "1b645767-ca4b-485c-b7a7-513066e4b1e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of biopsy:   7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6steming\n",
        "stemmer = nltk.PorterStemmer()\n",
        "#اخد كل lowercase واعمل لهمم\n",
        "#ثم اعمل لهم stem\n",
        "stem_tokens = lowercase_tokens\n",
        "stem_tokens[:] = [stemmer.stem(lt) for lt in lowercase_tokens]\n"
      ],
      "metadata": {
        "id": "fECMFazhqrWr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get freq of all lower then to know the fre of lower biopsy it repeated 7\n",
        "stem_freq = nltk.FreqDist(stem_tokens)\n",
        "#لانه بشيل بها الدايات وهنا كلمه اصلا كامله مفيش\n",
        "print(\"Frequency :  \", stem_freq[\"describes\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obsskYDdqzX8",
        "outputId": "7cfe1bca-3b68-4972-9932-0a16a1bd42cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency :   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7convert into sentenceجمل بقس انما الي فوق كلمات \n",
        "sents = nltk.sent_tokenize(text)\n",
        "#first sentence into sentence\n",
        "medical_tokens = nltk.word_tokenize(sents[1])\n",
        "\n",
        "medical_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYhQVFzQs8JS",
        "outputId": "987a2402-48f8-4c8d-9f52-d7f305cd397c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'biopsy',\n",
              " 'report',\n",
              " 'describes',\n",
              " 'the',\n",
              " 'findings',\n",
              " 'of',\n",
              " 'a',\n",
              " 'specimen',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can see many different POS tags, such as \"JJ\" and \"NN\".\n",
        "\n",
        "The meaning of commonly used POS tags is provided below:\n",
        "\n",
        "NN: noun\n",
        "JJ: adjective\n",
        "DT: determiner\n",
        "VB: verb in base form\n",
        "VBD: verb in past tense\n",
        "VBG: verb in gerund/present participle\n",
        "RB: adverb\n",
        "WRB wh-abverb (e.g. where)\n",
        "IN: preposition/subordinating conjunction\n",
        "CC: coordinating conjunction (e.g. and)"
      ],
      "metadata": {
        "id": "n4QMXBz0xGN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Part-of-Speech Tagging\n",
        "tagged = nltk.pos_tag(medical_tokens)\n",
        "tagged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67-3_S-Zt9oE",
        "outputId": "99cfaeb8-1ddf-4a77-dc5c-66e12a38eaa3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 'DT'),\n",
              " ('biopsy', 'JJ'),\n",
              " ('report', 'NN'),\n",
              " ('describes', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('findings', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('specimen', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Named Entity Recognition اعرف دا ايه تاريج ام اسك علي مانتج من خطو0 8\n",
        "\"\"\"Now that we have the parts of speech, we can try named entity recognition, which is concerned with the task of finding entities in text and classifying them as persons, locations, dates, and so on.\"\"\"\n",
        "entities = nltk.ne_chunk(tagged)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nsGbZkKuNi_",
        "outputId": "39b6bd30-47d0-4776-bdcb-fa2310751634"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  A/DT\n",
            "  biopsy/JJ\n",
            "  report/NN\n",
            "  describes/VBZ\n",
            "  the/DT\n",
            "  findings/NNS\n",
            "  of/IN\n",
            "  a/DT\n",
            "  specimen/NNS\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You could have a play with other sentences in the Medical Note document. Or you could try POS tagging for the Biopsy Report!\n",
        "\n",
        "\n",
        "sents = nltk.sent_tokenize(text)\n",
        "#first sentence into sentence\n",
        "medical_tokens = nltk.word_tokenize(sents[1])\n",
        "\n",
        "medical_tokens\n",
        "\n",
        "tagged = nltk.pos_tag(medical_tokens)\n",
        "tagged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOP5JJfVxfJi",
        "outputId": "62e1abdf-b182-41d8-d950-0f3ba7c77a0f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 'DT'),\n",
              " ('biopsy', 'JJ'),\n",
              " ('report', 'NN'),\n",
              " ('describes', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('findings', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('specimen', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}